# Introduction

# (PART) Foundations {-} 

# Names and values
## Prerequisites {-}

We use the development version of lobstr to answer questions regarding the internal representation of R objects.

```{r}
# devtools::install_github("r-lib/lobstr")
library(lobstr) 
```

## Binding basics

1. __<span style="color:red">Q</span>__: Explain the relationship between `a`, `b`, `c` and `d` in the following code:

    ```{r}
    a <- 1:10
    b <- a
    c <- b
    d <- 1:10
    ```
    
   __<span style="color:green">A</span>__: `a`, `b`, `c` point to the same object (with the same address in memory). This object has the value `1:10`. `d` points to a different object with the same value.

    ```{r}
    list_of_names <- list(a, b, c, d)
    obj_addrs(list_of_names)
    ```

2. __<span style="color:red">Q</span>__: The following code accesses the mean function in multiple ways. Do they all point to the same underlying function object? Verify with `lobstr::obj_addr()`.
    
    ```{r, eval = FALSE}
    mean
    base::mean
    get("mean")
    evalq(mean)
    match.fun("mean")
    ```
    
   __<span style="color:green">A</span>__: Yes, they point to the same object. We confirm this by inspecting the address of the underlying function object.
       
    ```{r}
    mean_functions <- list(mean,
                           base::mean,
                           get("mean"),
                           evalq(mean),
                           match.fun("mean"))
    
    unique(obj_addrs(mean_functions))
    ```
    
3. __<span style="color:red">Q</span>__: By default, base R data import functions, like `read.csv()`, will automatically convert non-syntactic names to syntactic names. Why might this be problematic? What option allows you to suppress this behaviour?
    
   __<span style="color:green">A</span>__: When automatic and implicit (name) conversion occurs, the prediction of a scripts output will be more difficult. For example when R is used non-interactively and some data is read, transformed and written, than the output may not contain the same names as the original data source. This behaviour may introduce problems in downstream analysis. To avoid automatic name conversion set `check.names=FALSE`.
    
4. __<span style="color:red">Q</span>__: What rules does `make.names()` use to convert non-syntactic names into syntactic names?
    
   __<span style="color:green">A</span>__: A valid name starts with a letter or a dot (which must not be followed by a number). It also consists of letters, numbers, dots and underscores only (`"_"` are allowed since R version 1.9.0).
   
   Three main mechanisms ensure syntactically valid names (see `?make.names`):
   - The variable name will be prepended by an `X` when names do not start with a letter or start with a dot followed by a number
    
      ```{r}
      make.names("")
      make.names(".1")
      ```
    
   - (additionally) non-valid characters are replaced by a dot
    
      ```{r}
      make.names("@")          # prepending + . replacement 
      make.names("  ")         # prepending + .. replacement
      make.names("non-valid")  # . replacement
      ```
    
   - reserved R keywords (see `?reserved`) are appended by a dot
    
      ```{r}
      make.names("if")
      ```
    
   Interestingly, some of these transformations are influenced by the current locale (from `?make.names`):

   > The definition of a letter depends on the current locale, but only ASCII digits are considered to be digits.

5. __<span style="color:red">Q</span>__: I slightly simplified the rules that govern syntactic names. Why is `.123e1` not a syntactic name? Read `?make.names` for the full details.
    
   __<span style="color:green">A</span>__: `.123e1` is not a syntactic name, because it starts with one dot which is followed by a number.

## Copy-on-modify

1. __<span style="color:red">Q</span>__: Why is `tracemem(1:10)` not useful?

   __<span style="color:green">A</span>__: 
   When `1:10` is called an object with an address in memory is created, but it is not bound to a name. Therefore the object cannot be called or manipulated from R. As no copies will be made, it is not useful to track the object for copying.
   
    ```{r}
    obj_addr(1:10)  # the object exists, but no copy will be made
    ```
   
2. __<span style="color:red">Q</span>__: Explain why `tracemem()` shows two copies when you run this code. Hint: carefully look at the difference between this code and the code show earlier in the section.
     
    ```{r, results = FALSE}
    x <- c(1L, 2L, 3L)
    tracemem(x)
    
    x[[3]] <- 4
    ```
    
   __<span style="color:green">A</span>__: Initially the vector `x` has integer type. The replacement call assigns a double to the third element of `x`, which triggers copy-on-modify. Because of R's coercion rules, a type conversion occurs, which affects the vector as a whole and leads to an additional copy.

    ```{r, eval=FALSE}
    # two copies
    x <- 1:3
    tracemem(x)
    #> <0x66a4a70>
    
    x[[3]] <- 4
    #> tracemem[0x55eec7b3af38 -> 0x55eec774cc18]: 
    #> tracemem[0x55eec774cc18 -> 0x55eeca6ed5a8]: 
    ```
    
   By assigning an integer instead of a double one copy (the one related to coercion) may be avoided:
    
    ```{r, eval=FALSE}
    # the same as 
    x <- 1:3
    tracemem(x)
    #> <0x55eec6940ae0>
    
    x[[3]] <- 4L
    #> tracemem[0x55eec7021e10 -> 0x55eecb99e788]: 
    x <- as.double(x)
    #> tracemem[0x55eecb99e788 -> 0x55eec93d9c18]:
    ```

   <!-- code-chunks above were hard-coded to fix knitr output. -->

3. __<span style="color:red">Q</span>__: Sketch out the relationship between the following objects:

    ```{r}
    a <- 1:10
    b <- list(a, a)
    c <- list(b, a, 1:10)
    ```
    
   __<span style="color:green">A</span>__: `a` contains a reference to an address with the value `1:10`. `b` contains a list of two references to the same address as `a`. `c` contains a list of `b` (containing two references to `a`), `a` (containing the same reference again) and a reference pointing to a different address containing the same value (`1:10`).

    ```{r, eval = FALSE}
    ref(c)
    #> █ [1:0x55eec93cbdd8] <list>    # c
    #> ├─█ [2:0x55eecb8246e8] <list>  # - b
    #> │ ├─[3:0x55eec7df4e98] <int>   # -- a
    #> │ └─[3:0x55eec7df4e98]         # -- a
    #> ├─[3:0x55eec7df4e98]           # - a
    #> └─[4:0x55eec7aa6968] <int>     # - 1:10
    ```


4. __<span style="color:red">Q</span>__: What happens when you run this code:

    ```{r}
    x <- list(1:10)
    x[[2]] <- x
    ```
    
   Draw a picture.

   __<span style="color:green">A</span>__: The initial reference tree of `x` shows, that the name `x` binds to a list object. This object contains a reference to the integer vector `1:10`.

    ```{r, eval=FALSE}
    x <- list(1:10)
    ref(x)
    #> █ [1:0x55853b74ff40] <list> 
    #> └─[2:0x534t3abffad8] <int> 
    ```

    ```{r, echo = FALSE}
    knitr::include_graphics("images/copy_on_modify_fig1.png")
    ```

   When `x` is assigned to an element of itself copy-on-modify takes place and the list is copied to a new address in memory.

    ```{r, eval=FALSE}
    tracemem(x)
    x[[2]] <- x
    #> tracemem[0x55853b74ff40 -> 0x5d553bacdcd8]:
    ```

   The list object previously bound to `x` is now referenced in the newly created list object. It is no longer bound to a name. The integer vector is referenced twice.

    ```{r, eval=FALSE}
    ref(x)
    #> █ [1:0x5d553bacdcd8] <list> 
    #> ├─[2:0x534t3abffad8] <int> 
    #> └─█ [3:0x55853b74ff40] <list> 
    #>   └─[2:0x534t3abffad8] 
    ```

    ```{r, echo = FALSE}
    Sys.sleep(0.1)
    knitr::include_graphics("images/copy_on_modify_fig2.png")
    ```

## Object size

1. __<span style="color:red">Q</span>__: In the following example, why are `object.size(y)` and `obj_size(y)` so radically different? Consult the documentation of `object.size()`.

    ```{r}
    y <- rep(list(runif(1e4)), 100)
    
    object.size(y)
    obj_size(y)
    ```
    
   __<span style="color:green">A</span>__: `object.size()` doesn't account for shared elements within lists. Therefore, the results differ by a factor of ~ 100.

2. __<span style="color:red">Q</span>__: Take the following list. Why is its size somewhat misleading?

    ```{r, return = FALSE}
    x <- list(mean, sd, var)
    # obj_size(x)
    #> 16,928 B
    ```
    __<span style="color:green">A</span>__: It is somewhat misleading, because all three functions are built-in to R as part of the base and stats packages and hence always loaded. 
    
   From the following calculations we can see that this applies to about `r base_env_names <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "Autoloads", "package:base"); base_env_list <- sapply(base_env_names, function(x) mget(ls(x, all = TRUE), as.environment(x))); sum(lengths(base_env_list))` objects which are usually loaded by default and take up about `r round(sum(sapply(base_env_list, lobstr::obj_size)) / 1024^2, 2)` MB of memory.
  
    ```{r}
    base_env_names <- c("package:stats", "package:graphics", "package:grDevices",
                        "package:utils", "package:datasets", "package:methods"  ,
                        "Autoloads"    , "package:base")
    
    base_env_list <- sapply(base_env_names, 
                            function(x) mget(ls(x, all = TRUE), as.environment(x)))
    
    sum(lengths(base_env_list))
    
    sapply(base_env_list, lobstr::obj_size)
    round(sum(sapply(base_env_list, lobstr::obj_size)) / 1024^2, 2)
    ```
  
3. __<span style="color:red">Q</span>__: Predict the output of the following code:

    ```{r, eval = FALSE}
    x <- runif(1e6)
    obj_size(x)
    
    y <- list(x, x)
    obj_size(y)
    obj_size(x, y)
    
    y[[1]][[1]] <- 10
    obj_size(y)
    obj_size(x, y)
    
    y[[2]][[1]] <- 10
    obj_size(y)
    obj_size(x, y)
    ```
    
   __<span style="color:green">A</span>__: To predict the object size of `x` let's first find out how much memory an empty double occupies and how the size grows with the length of the vector.
   
    ```{r}
    # memory needed for a 0-length double
    obj_size(double(0))
    
    # additional memory per element
    diff(sapply(0:32, function(x) obj_size(double(x))))
    ```
    
   We see that R requires 48 bytes for a double of length 0. Generally each additional element in a vector requires 8 additional bytes of memory. But for some small vectors R preallocates a little more memory than needed, which improves performance. Here is Hadley´s explanation from the first edition of Advanced R:
   
   > ... why does the memory size grow irregularly? To understand why, you need to know a little bit about how R requests memory from the operating system. Requesting memory (with malloc()) is a relatively expensive operation. Having to request memory every time a small vector is created would slow R down considerably. Instead, R asks for a big block of memory and then manages that block itself. This block is called the small vector pool and is used for vectors less than 128 bytes long. For efficiency and simplicity, it only allocates vectors that are 8, 16, 32, 48, 64, or 128 bytes long. ... Beyond 128 bytes, it no longer makes sense for R to manage vectors. After all, allocating big chunks of memory is something that operating systems are very good at. Beyond 128 bytes, R will ask for memory in multiples of 8 bytes. This ensures good alignment.
   
   However, to estimate `obj_size(x)` we just calculate _48 B + 1000000 * 8 B = 8000048 B_ (about 8 MB), which proves correct:
   
    ```{r}
    x <- runif(1e6)
    obj_size(x)
    ```
   
   In `y <- list(x, x)` both list elements of `y` contain references to the same memory address, so no additional memory is required for the second list element. The list itsself requires 64 bytes, 48 byte for an empty list and 8 byte for each element (`obj_size(vector("list", 2))`). This let's us predict 8000048 B + 64 B = 8000112 B:
   
    ```{r}
    y <- list(x, x) 
    obj_size(y)
    ```
   
   The list `y` already contains references to `x`, so no extra memory is needed for `x` and the amount of required memory stays the same.
   
    ```{r}
    obj_size(x, y) 
    ```
    
   When we modify the first element of `y[[1]]` copy-on-modify occurs and the object will have the same size (8000040 bytes) and a new address in memory. So `y`'s elements don't share references anymore. Because of this their object sizes add up to the sum of of the two different vectors and the length-2 list: 8000048 B + 8000048 B + 64 B = 16000160 B (16 MB).

    ```{r}
    y[[1]][[1]] <- 10
    obj_size(y) 
    ```
    
   The second element of `y` still references to the same address as `x` and therefore the amount of memory used for both `x` and `y` doesn´t increase:
   
    ```{r}
    obj_size(x, y) 
    ```
    
   When we modify the second element of `y`, this element will also point to a new memory address. This doesn´t affect the memory size of the list:
   
    ```{r}
    y[[2]][[1]] <- 10
    obj_size(y) 
    ```
   
   However, as `y` doesn't share references with `x` anymore, the memory usage of the objects now adds up:
     
    ```{r}
    obj_size(x, y) 
    ```

## Modify-in-place

1. __<span style="color:red">Q</span>__: Explain why the following code doesn't create a circular list.

    ```{r}
    x <- list()
    x[[1]] <- x
    ```
    
   __<span style="color:orange">A</span>__: In this situation Copy-on-modify prevents the creation of a circular list. Let's step through the details as follows:
   
    ```{r}
    x <- list()      # creates initial object
    obj_addr(x)
    
    tracemem(x)
    x[[1]] <- x  # Copy-on-modify triggers new copy
    
    obj_addr(x)       # copied object has new memory address
    obj_addr(x[[1]])  # list element contains old memory adress
    ```

2. __<span style="color:red">Q</span>__: Wrap the two methods for subtracting medians into two functions, then use the bench package to carefully compare their speeds. How does performance change as the number of columns increase?
    
   __<span style="color:green">A</span>__: First, let's define a function to create some random data and a function to subtract the median from each column.

    ```{r}
    create_random_df <- function(nrow, ncol) {
      random_matrix <- matrix(runif(nrow * ncol), nrow = nrow)
      as.data.frame(random_matrix)
    }
    
    subtract_medians <- function(x, medians){
      for (i in seq_along(medians)) {
        x[[i]] <- x[[i]] - medians[[i]]
      }
      x
    }
    ```

   We can then profile the performance, by benchmarking `subtact_medians()` on data frame- and list-input for a specified number of columns. The functions should both input and output a data frame, so one is going to do a bit more work.

    ```{r}
    compare_speed <- function(ncol){
      df_input   <- create_random_df(nrow = 1e4, ncol = ncol)
      medians <- vapply(df_input, median, numeric(1))
      
      bench::mark(`Data Frame` = subtract_medians(df_input,   medians),
                  List = as.data.frame(subtract_medians(as.list(df_input), medians)))
    }
    ```

   Then bench package allows us to run our benchmark across a grid of parameters easily. We will use it to slowly increase the number of columns containing random data.

    ```{r, warning=FALSE, message=FALSE, eval=FALSE}
    results <- bench::press(
      ncol = c(1, 5, 10, 50, 100, 200, 400, 600, 800, 1000, 1500),
      compare_speed(ncol)
    )
    
    library(ggplot2)
    ggplot(results, aes(ncol, median, col = expression)) +
      geom_point(size = 2) + 
      geom_smooth() +
      labs(x = "Number of Columns of Input Data", y = "Computation Time",
           color = "Input Data Structure",
           title = "Benchmark: Median Subtraction")
    ```

    ```{r, echo = FALSE}
    knitr::include_graphics("images/median_benchmark.png")
    ```

   The execution times for median subtraction on data frames columns increase exponentially with the number of columns in the input data. This is because, the data frame will be copied more often and the copy will also be bigger. For subtraction on list elements the execution time increases only linearly.
   
   For list input with less than ~ 800 columns, the cost of the additional data structure conversion is relatively big. For very wide data frames the overhead of the additional copies slows down the computation considerably. Apparently the choice of the faster function depends on the size of the data also.

3. __<span style="color:red">Q</span>__: What happens if you attempt to use `tracemem()` on an environment?

   __<span style="color:green">A</span>__: `tracemem()` cannot be used to mark and trace environments.

    ```{r, error=TRUE}
    x <- new.env()
    tracemem(x)
    ```

   The error occurs because "it is not useful to trace NULL, environments, promises, weak references, or external pointer objects, as these are not duplicated" (see `?tracemem`). Environments are always modified in place.
